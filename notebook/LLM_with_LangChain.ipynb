{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc1b2cb-97d7-4cdb-9db1-04dc7fa2b7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:25:58.393574: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-07 17:25:58.561412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-07 17:25:59.449464: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import getpass\n",
    "import os \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986c42ad-c48d-4448-aecc-2b663ac7f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokeys import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef60114-7882-4b46-abfd-f309e9569d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618ca7d9-ddd9-440a-9fa8-bd4fcd4ec618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aff1a0912e34c4681540ba5f365ff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config = quantization_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce0f7ad-f68a-4152-ae7e-1c004e7ba1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    task = \"text-generation\",\n",
    "    temperature = 0.1,\n",
    "    max_new_tokens = 500,\n",
    "    do_sample = True, \n",
    "    repetition_penalty = 1.1,\n",
    "    return_full_text = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a56a53f-4f15-46d3-88c0-2c076d7376bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Answer:A primeira pessoa no espaço foi Yuri Gagarin, um cosmonauta soviético. Ele voou em uma missão orbital ao redor da Terra como parte do programa espacial soviético Vostok 1. Sua viagem ocorreu de 12 de abril de 1961 e durou cerca de 108 minutos. Ao retornar à Terra, ele pousou com segurança na Rússia, marcando assim o primeiro pouso humano bem-sucedido após um voo espacial tripulado.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "input_ = \"Quem foi a primeira pessoa no espaço?\"\n",
    "output = llm.invoke(input_)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbffb8a-63b9-4ad2-b762-e7b5c63043af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
