{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3267b9ba-6e7c-406e-b9ac-7975b776d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -q transformers==4.48.2 einops accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9de0d-9463-4f98-9935-5c7c3205796f",
   "metadata": {},
   "source": [
    "## Importando as seguintes bibliotecas\n",
    "* <b>AutoModelForCausalLM:</b> Uma classe que fornece um modelo de linguagem causal (ou autoregressivo) pré-treinado (por exemplo, GPT-2, GPT-3) que são adequados para tarefas de geração de texto.\n",
    "* <b>AutoTokenizer:</b> Uma classe que fornece um tokenizador que corresponde ao modelo. O tokenizador é responsável por converter texto em tokens (numéricos) que o modelo pode entender.\n",
    "* <b>pipeline:</b> fornece uma interface simples e unificada para várias tarefas de PLN, facilitando a execução de tarefas como geração de texto, classificação e tradução.\n",
    "* <b>BitsAndBytesConfig:</b> Uma classe para configuração de quantização e outras otimizações de baixo nível para melhorar a eficiência computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4b5163-ccad-4ab3-9d51-f5a3ba8be52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 19:50:07.638120: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-02 19:50:07.665228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-02 19:50:08.412694: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from yaml import safe_load\n",
    "import torch\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36150ca-1c80-4552-900f-889015e130ee",
   "metadata": {},
   "source": [
    "## Leitura da chave do Hugging Face e verificando se o cuda está ativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67aa533e-c02f-4285-89b4-98c8d64b6d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda', {'HUGGING_API_TOKEN': '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'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'key.yaml'\n",
    "\n",
    "with open(PATH) as token_file:\n",
    "    token  = safe_load(token_file)\n",
    "\n",
    "os.environ['HUGGING_API_TOKEN'] = token['HUGGING_API_TOKEN']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device, token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0b1785-13bb-4b94-ab00-44d354f203bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d5ab779c630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee433d-f155-4ef0-9822-c9dbf6e70a3c",
   "metadata": {},
   "source": [
    "## Carregando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e812671-fe0f-4b89-8cb9-b23e0a2c0027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c0e67e-1693-4e0a-b7fe-ee89a004d153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acd3750234c4f0e8e6c86566ceaf24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3903142f08a415cae021dd8833f51bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e02dfa48054eeca92318bfa610a3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018b5d5c0e194c3cb5d97617ad2340bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae79ea464678449a963dc0410d191c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d1f111ab6b4d7abe3a98ec1464765c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9616b3cf0e4436a2a8a4f2bbc103ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125b9751266548cc8e944eca441b59a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4117e328454d89be48805b3590e278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_model,            \n",
    "    device_map = device,        # Para acelerar o processo, o modelo usará a placa de vídeo (GPU) que o coputador oferece ou mesmo CPU.\n",
    "    torch_dtype = \"auto\",       # Define o tipo de dado ideal para o modelo rodar mais rápido e usar menos memória.\n",
    "    trust_remote_code = True,   # Permite usar código personalizado do Hugging Face, necessário para modelos específicos.\n",
    "    attn_implementation=\"eager\" # Escolhe um método de atenção (\"eager\") para melhorar o desempenho de alguns modelos.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2904e-f4ed-430b-8bfd-06e003627de3",
   "metadata": {},
   "source": [
    "##  Criando o Tokinizador \n",
    "\n",
    "<p>O tokenizador traduz o texto em números (tokens) para que o modelo de IA consiga processá-lo.</br> Em seguida, ele converte os números gerados pelo modelo de volta em texto que nós podemos entender.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d84605-f8eb-4b75-89ea-c839f8a6e74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ece8b796f24e549c4bbfbc046b4b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8893948891d946b0b589040ef043320f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da48f393c0f64fca9ddf9e6d77f09b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32614d63aad24253bec4f0f039824656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda04621ba9c439ca0719f79b28dcbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b527905-d895-43e5-8201-912a65ac4ac8",
   "metadata": {},
   "source": [
    "## Criação do Pipeline\n",
    "\n",
    "Parâmetros:\n",
    "\n",
    "* \"text-generation\": especifica a tarefa que o pipeline está configurado para executar. Neste caso, estamos configurando um pipeline para geração de texto. O pipeline usará o modelo para gerar texto com base em um     prompt fornecido.\n",
    "* model=model: especifica o modelo pré-treinado que o pipeline usará. Aqui, estamos passando o model que carregamos anteriormente. Este modelo é responsável por gerar texto com base nos tokens de entrada.\n",
    "* tokenizer=tokenizer: especifica o tokenizador que o pipeline usará. Passamos o tokenizer que carregamos anteriormente para garantir que o texto de entrada seja tokenizado corretamente e os tokens de saída sejam     decodificados com precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee6e2ef-1787-4559-b297-22dad5bd898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model, \n",
    "    tokenizer= tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c732544-0df0-491a-af71-5f224b262936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42371ec-7e6d-4d9f-a858-83005aa15cc2",
   "metadata": {},
   "source": [
    "## Parâmetros para geração de texto\n",
    "\n",
    "Para personalizar o comportamento do nosso pipeline de geração de texto, podemos passar um dicionário de argumentos para controlar vários aspectos do processo de geração.\n",
    "\n",
    "`max_new_tokens` - Este parâmetro especifica o número máximo de novos tokens a serem gerados em resposta ao prompt de entrada. Ele controla o comprimento do texto gerado.\n",
    "* Exemplo: Definir max_new_tokens como 500 significa que o modelo gerará até 500 tokens além do prompt de entrada.\n",
    "\n",
    "`return_full_text` - Determina se deve retornar o texto completo, incluindo o prompt de entrada, ou apenas os tokens recém-gerados.\n",
    "* Exemplo: Definir return_full_text como False significa que apenas os tokens recém-gerados serão retornados, excluindo o prompt de entrada original. Se definido como True, o texto retornado incluirá o prompt de entrada e a continuação gerada.\n",
    "\n",
    "`temperature` - Controla a aleatoriedade do processo de geração de texto. Valores mais baixos tornam a saída do modelo mais determinística e focada, enquanto valores mais altos aumentam a aleatoriedade e a criatividade.\n",
    "* Exemplo: Uma temperatura de 0,1 torna as previsões do modelo mais confiáveis e menos variadas, levando a saídas mais previsíveis. Uma temperatura mais alta resultaria em um texto mais diverso e variado.\n",
    "\n",
    "`do_sample` - Este parâmetro habilita ou desabilita a amostragem (sampling) durante a geração de texto. Quando definido como True, o modelo faz a amostragem de tokens com base em suas probabilidades, adicionando um elemento de aleatoriedade à saída. Quando definido como False, o modelo sempre escolhe o token de maior probabilidade (decodificação gananciosa / greedy decoding).\n",
    "* Exemplo: Definir do_sample como True permite uma geração de texto mais diversa e criativa. Se definido como False, a saída será mais determinística, mas potencialmente menos interessante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7346066-1863-419b-9b13-88282b59abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.1, # 0.1 até 0.9\n",
    "    \"do_sample\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12cfbe90-4a67-462b-bf25-eaebd9164098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4f9491-215b-499c-b448-7a56e14e4207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Explique o que é computação quântica\"\n",
    "prompt = \"Quanto é 7 x 6 - 42?\"\n",
    "\n",
    "output = pipe(prompt, **generation_args);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1b9c175-0c43-4b9f-ab4c-52a554af7c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'generated_text': '\\n\\nOpções de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\\n\\n\\n### Answer\\n\\nPara resolver a expressão 7 x 6 - 42, precisamos seguir a ordem das operações, que é frequentemente lembrada pelo acrônimo PEMDAS (Parênteses, Expoentes, Multiplicação e Divisão, e Adição e Subtração). Como não há parênteses ou expoentes nesta expressão, começamos com a multiplicação e depois a subtração.\\n\\nPasso 1: Realizar a multiplicação\\n7 x 6 = 42\\n\\nPasso 2: Subtrair o resultado da multiplicação de 42\\n42 - 42 = 0\\n\\nPortanto, a resposta para a expressão 7 x 6 - 42 é 0.'}],\n",
       " list)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f67178f-481d-430f-9c4c-bd8c81013738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Opções de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\n",
      "\n",
      "\n",
      "### Answer\n",
      "\n",
      "Para resolver a expressão 7 x 6 - 42, precisamos seguir a ordem das operações, que é frequentemente lembrada pelo acrônimo PEMDAS (Parênteses, Expoentes, Multiplicação e Divisão, e Adição e Subtração). Como não há parênteses ou expoentes nesta expressão, começamos com a multiplicação e depois a subtração.\n",
      "\n",
      "Passo 1: Realizar a multiplicação\n",
      "7 x 6 = 42\n",
      "\n",
      "Passo 2: Subtrair o resultado da multiplicação de 42\n",
      "42 - 42 = 0\n",
      "\n",
      "Portanto, a resposta para a expressão 7 x 6 - 42 é 0. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb99a78-b00f-43da-bf59-dacb484498a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "### Answer:O Brasil é o país mais populoso da América do Sul e o quarto maior do mundo, com uma população estimada em mais de 210 milhões de pessoas. É conhecido por sua rica biodiversidade, com uma grande variedade de paisagens, desde as florestas tropicais da Amazônia até as planícies do Pantanal e as montanhas da Serra do Espinhaço.\n",
      "\n",
      "A economia do Brasil é diversificada, com setores importantes incluindo agricultura, mineração, manufatura e serviços. O país é um dos maiores produtores de açúcar, soja, café e algodão, e também é um importante exportador de minérios como ferro, cobre e ouro.\n",
      "\n",
      "A cultura brasileira é vibrante e influenciada por uma mistura de tradições indígenas, africanas e europeias. A música, como samba, bossa nova e pagode, é mundialmente famosa, e o país também é conhecido por seus desfiles de carnaval, que são celebrados em todo o país.\n",
      "\n",
      "A política brasileira é caracterizada por um sistema de governo presidencialista, com o presidente como chefe de estado e governo. O país tem um histórico complexo, com períodos de prosperidade e instabilidade, mas recentemente tem feito progressos significativos em áreas como educação, saúde e desenvolvimento econômico.\n",
      "\n",
      "O Brasil também é conhecido por suas belezas naturais, incluindo a Praia do Forte, a Floresta Amazônica e a Serra do Caparaó. O país tem uma rica história e uma cultura diversificada, tornando-o um destino popular para turistas de todo o mundo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Me fale um pouco do Brasil\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "\n",
    "print(f\"{output[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be028c8-d55b-413c-a4b1-42a26906af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os tokens de paradas\n",
    "# obs: para cada modeo é aconselhável ver a documentação do modelo e sobre o tokens de parada ( end token )\n",
    "# Para este modelo vamos utilizar este tokens de parada\n",
    "\n",
    "\n",
    "# prompt = \"Explique o que é computação quântica\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "Você é um assistente de ajuda.<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(prompt) # formata o prompt para o formato adequado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc85d50-8fb1-43c4-a6d4-c2443d53f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Você é um assistente de ajuda.<|end|>\n",
      "<|user|>\n",
      "\"Me fale um pouco do Brasil\"<|end|>\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f77c5c-60fe-419e-a51f-10ecbb798ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nVocê é um assistente de ajuda.<|end|>\\n<|user|>\\n\"Me fale um pouco do Brasil\"<|end|>\\n<|assistant|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "899440d7-e982-4284-a419-96cf13a6aff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O Brasil é o maior país da América do Sul e o quarto maior do mundo em extensão territorial, com uma área de aproximadamente 8.515.767 quilômetros quadrados. Sua capital é Brasília, inaugurada em 1960, e sua maior cidade é São Paulo, conhecida por sua rica história, cultura e economia dinâmica. O país é conhecido por sua diversidade cultural, com mais de 200 grupos indígenas e uma população de mais de 210 milhões de pessoas. O Brasil tem uma economia vibrante e é um dos maiores produtores de açúcar, café, soja e petróleo do mundo. Além disso, o país é famoso por seus festivais, como o Carnaval, que é celebrado em todo o país e é considerado um dos maiores eventos culturais do mundo.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab85c65-4757-4f33-821c-bc45e5c4c69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "665c9733-41bd-430a-acfd-798a2264ecc5",
   "metadata": {},
   "source": [
    "## Explorando a engenharia de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4cab414-8a99-42db-a06f-face29ed7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Você é um assistente virtual prestativo. Responda as perguntas em português.<|end|>\n",
      "<|user|>\n",
      "\"Descreva quais são as principais bigtechs do mercado global nos dias tuais.\"<|end|>\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Descreva quais são as principais bigtechs do mercado global nos dias tuais.\"\n",
    "sys_prompt = \"Você é um assistente virtual prestativo. Responda as perguntas em português.\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4ea6a7-53a2-4bb4-9226-ad65f4862b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As principais bigtechs do mercado global são empresas que operam em setores como tecnologia da informação, internet e inteligência artificial, e que têm uma influência significativa em diversos aspectos da economia e da sociedade. Algumas das maiores e mais influentes bigtechs incluem:\n",
      "\n",
      "\n",
      "1. **Google**: Fundada em 1998, a Google é uma empresa multinacional que desenvolve e vende produtos e serviços relacionados à internet, incluindo navegadores de internet, sistemas operacionais, software de produtividade, e-mail, serviços de busca, entre outros.\n",
      "\n",
      "\n",
      "2. **Facebook (Meta Platforms)**: Fundada em 2004, o Facebook é uma plataforma de rede social que também oferece serviços como Instagram, WhatsApp e Oculus VR. A Meta Platforms, que é o nome atual da empresa após a mudança de nome em 2021, está focada na realidade virtual e na tecnologia Meta.\n",
      "\n",
      "\n",
      "3. **Amazon**: Fundada em 1994, a Amazon é uma empresa de comércio eletrônico que oferece uma ampla gama de produtos e serviços, incluindo varejo, serviços de streaming, serviços de computação em nuvem, ecologia, entre outros.\n",
      "\n",
      "\n",
      "4. **Apple**: Fundada em 1976, a Apple é uma empresa que desenvolve e vende produtos eletrônicos, incluindo smartphones, tablets, computadores pessoais, e serviços de software e hardware.\n",
      "\n",
      "\n",
      "5. **Microsoft**: Fundada em 1975, a Microsoft é uma empresa que desenvolve e vende software, serviços de computação em nuvem, jogos eletrônicos, e hardware.\n",
      "\n",
      "\n",
      "Essas empresas são consideradas bigtechs devido à sua grande escala, poder de mercado, impacto econômico e influência social. Elas têm sido alvo de escrutínio regulatório e de discussões sobre seu poder e responsabilidade no mercado global.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ddbd4f9-8b03-4295-911d-9dcb71b9f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aqui está um exemplo de código em Rust que gera a sequência de Fibonacci até um número específico:\n",
      "\n",
      "```rust\n",
      "fn main() {\n",
      "    let n = 10; // Número de elementos da sequência de Fibonacci\n",
      "    let mut fib = vec![0, 1]; // Inicializa a sequência com os dois primeiros elementos\n",
      "\n",
      "    for i in 2..n {\n",
      "        let next_element = fib[i - 1] + fib[i - 2]; // Calcula o próximo elemento da sequência\n",
      "        fib.push(next_element); // Adiciona o elemento calculado à sequência\n",
      "    }\n",
      "\n",
      "    println!(\"Sequência de Fibonacci até {} elementos: {:?}\", n, fib);\n",
      "}\n",
      "```\n",
      "\n",
      "Explicação:\n",
      "\n",
      "- Primeiro, definimos o número de elementos da sequência de Fibonacci que queremos gerar, neste caso, 10.\n",
      "- Em seguida, inicializamos a sequência com os dois primeiros elementos da sequência de Fibonacci, que são 0 e 1.\n",
      "- Em seguida, usamos um loop for para calcular os elementos restantes da sequência. O loop começa no índice 2, pois já temos os dois primeiros elementos.\n",
      "- Dentro do loop, calculamos o próximo elemento da sequência somando os dois elementos anteriores.\n",
      "- Em seguida, adicionamos o elemento calculado à sequência usando o método `push`.\n",
      "- Por fim, imprimimos a sequência gerada usando a função `println!`.\n",
      "\n",
      "Quando executamos este código, o resultado será:\n",
      "\n",
      "```\n",
      "Sequência de Fibonacci até 10 elementos: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Gere um código em Rust que escreva a sequência de fibonnaci\"\n",
    "\n",
    "sys_prompt = \"Você é um programador experiente. Retorne o código requisitado e forneça explicações.\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95fefc-86eb-4b7b-9532-d387a5e1aa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2f1890-5210-4204-8ec0-2e887e4ad053",
   "metadata": {},
   "source": [
    "# Formato de Mensagens (LLMs em Chat)\n",
    "\n",
    "**Ideia central:** Em chats com LLMs, a conversa é uma **lista de mensagens**.  \n",
    "Cada mensagem tem:\n",
    "- **`role`**: quem está falando (`system`, `user`, `assistant`)\n",
    "- **`content`**: o texto da mensagem\n",
    "\n",
    "## Por que usar esse formato?\n",
    "- O modelo entende **contexto de ida e volta** (vários turnos).\n",
    "- Facilita controlar o comportamento (ex.: instruções no `system`).\n",
    "\n",
    "## Estrutura do prompt (lista `msg`)\n",
    "Você envia algo como uma lista de dicionários com `role` e `content`.  \n",
    "O modelo responde levando em conta **toda a conversa**, com foco na **última** mensagem.\n",
    "\n",
    "### Exemplo mínimo\n",
    "```json\n",
    "[\n",
    "  { \"role\": \"system\", \"content\": \"Você é um assistente útil e direto.\" },\n",
    "  { \"role\": \"user\",   \"content\": \"Explique o que é uma mensagem.\" }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d57b9f63-1618-4cc2-9dee-56898b2371d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Um prompt é uma instrução ou solicitação que você dá a um modelo de IA, como um chatbot ou uma inteligência artificial, para que ele possa entender e responder adequadamente. É como uma dica ou um ponto de partida para a IA começar a processar a informação e gerar uma resposta.\n",
      "\n",
      "\n",
      "Por outro lado, um template é um modelo pré-definido que possui uma estrutura específica e pode ser usado para gerar respostas consistentes. Templas são úteis quando você precisa de respostas que seguam um formato específico, como um e-mail, um relatório ou uma mensagem. Eles ajudam a garantir que as informações sejam apresentadas de forma organizada e clara.\n",
      "\n",
      "\n",
      "Em resumo, um prompt é uma dica para a IA, enquanto um template é um modelo de resposta que ajuda a manter a consistência.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explique de uma forma simples o conceito de prompt e tamplet\"\n",
    "\n",
    "msg = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente virtual prestativo com conhecimentos e praticas de IA. Responda as perguntas em português.\" },\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "\n",
    "output = pipe(msg, **generation_args)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a0ab-f2df-45ab-956b-4c485771aee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
