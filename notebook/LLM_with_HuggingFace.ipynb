{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3267b9ba-6e7c-406e-b9ac-7975b776d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -q transformers==4.48.2 einops accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9de0d-9463-4f98-9935-5c7c3205796f",
   "metadata": {},
   "source": [
    "## Importando as seguintes bibliotecas\n",
    "* <b>AutoModelForCausalLM:</b> Utilizado para carregar um modelo pr√©-treinado para gerar texto (escrever hist√≥rias, responder perguntas, completar c√≥digo, etc.) e n√£o quer se preocupar em encontrar a classe espec√≠fica para aquele modelo.\n",
    "* <b>AutoTokenizer:</b> Uma classe que fornece um tokenizador que corresponde ao modelo. O tokenizador √© respons√°vel por converter texto em tokens (num√©ricos) que o modelo pode entender.\n",
    "* <b>pipeline:</b> fornece uma interface simples e unificada para v√°rias tarefas de PLN, facilitando a execu√ß√£o de tarefas como gera√ß√£o de texto, classifica√ß√£o e tradu√ß√£o.\n",
    "* <b>BitsAndBytesConfig:</b> Uma classe para configura√ß√£o de quantiza√ß√£o e outras otimiza√ß√µes de baixo n√≠vel para melhorar a efici√™ncia computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4b5163-ccad-4ab3-9d51-f5a3ba8be52e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/mborges/miniconda3/envs/IA/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tokens\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/IA/lib/python3.11/site-packages/transformers/__init__.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     29\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     logging,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     52\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/IA/lib/python3.11/site-packages/transformers/dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/IA/lib/python3.11/site-packages/transformers/utils/__init__.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     31\u001b[39m     add_end_docstrings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     replace_return_docstrings,\n\u001b[32m     36\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/IA/lib/python3.11/site-packages/transformers/utils/chat_template_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     43\u001b[39m BASIC_TYPES = (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), ...)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/IA/lib/python3.11/site-packages/torch/__init__.py:367\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    366\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    371\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: /home/mborges/miniconda3/envs/IA/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from keys import tokens\n",
    "import torch\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36150ca-1c80-4552-900f-889015e130ee",
   "metadata": {},
   "source": [
    "## Leitura da chave do Hugging Face e verificando se o cuda est√° ativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b331ef-0aae-4d40-bc69-4b5e33fae3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>TOKEN ACESSADO COM SUCESSO<<\n",
      "cuda , hf_ozQeB\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "T = tokens()\n",
    "print(device,\",\",T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061398a-b37f-4341-ba5a-7be89840eec3",
   "metadata": {},
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0b1785-13bb-4b94-ab00-44d354f203bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f155ddb4ad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee433d-f155-4ef0-9822-c9dbf6e70a3c",
   "metadata": {},
   "source": [
    "## Carregando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e812671-fe0f-4b89-8cb9-b23e0a2c0027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c0e67e-1693-4e0a-b7fe-ee89a004d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m id_model = \u001b[33m\"\u001b[39m\u001b[33mmicrosoft/Phi-3-mini-4k-instruct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Para acelerar o processo, o modelo usar√° a placa de v√≠deo (GPU) que o coputador oferece ou mesmo CPU.\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Define o tipo de dado ideal para o modelo rodar mais r√°pido e usar menos mem√≥ria.\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Permite usar c√≥digo personalizado do Hugging Face, necess√°rio para modelos espec√≠ficos.\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meager\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Escolhe um m√©todo de aten√ß√£o (\"eager\") para melhorar o desempenho de alguns modelos.\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/IA/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:559\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    558\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/IA/lib/python3.11/site-packages/transformers/modeling_utils.py:3535\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3531\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3532\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3533\u001b[39m         )\n\u001b[32m   3534\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m3535\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   3536\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3537\u001b[39m         )\n\u001b[32m   3539\u001b[39m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[32m   3540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[31mImportError\u001b[39m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "id_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_model,            \n",
    "    device_map = device,        # Para acelerar o processo, o modelo usar√° a placa de v√≠deo (GPU) que o coputador oferece ou mesmo CPU.\n",
    "    torch_dtype = \"auto\",       # Define o tipo de dado ideal para o modelo rodar mais r√°pido e usar menos mem√≥ria.\n",
    "    trust_remote_code = True,   # Permite usar c√≥digo personalizado do Hugging Face, necess√°rio para modelos espec√≠ficos.\n",
    "    attn_implementation=\"eager\" # Escolhe um m√©todo de aten√ß√£o (\"eager\") para melhorar o desempenho de alguns modelos.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2904e-f4ed-430b-8bfd-06e003627de3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Criando o Tokinizador \n",
    "\n",
    "<p>O tokenizador traduz o texto em n√∫meros (tokens) para que o modelo de IA consiga process√°-lo.</br> Em seguida, ele converte os n√∫meros gerados pelo modelo de volta em texto que n√≥s podemos entender.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d84605-f8eb-4b75-89ea-c839f8a6e74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220bd6ff5181494a9276ec00828f81e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bf3ac956a44959989fff62da386fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9986107da1cc43eb936c289717366ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e483d6799c0d4264b6c5c56144ca5a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dc35514f3e4e5bb1f82f3c9c5437e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b527905-d895-43e5-8201-912a65ac4ac8",
   "metadata": {},
   "source": [
    "## Cria√ß√£o do Pipeline\n",
    "\n",
    "Par√¢metros:\n",
    "\n",
    "* \"text-generation\": especifica a tarefa que o pipeline est√° configurado para executar. Neste caso, estamos configurando um pipeline para gera√ß√£o de texto. O pipeline usar√° o modelo para gerar texto com base em um     prompt fornecido.\n",
    "* model=model: especifica o modelo pr√©-treinado que o pipeline usar√°. Aqui, estamos passando o model que carregamos anteriormente. Este modelo √© respons√°vel por gerar texto com base nos tokens de entrada.\n",
    "* tokenizer=tokenizer: especifica o tokenizador que o pipeline usar√°. Passamos o tokenizer que carregamos anteriormente para garantir que o texto de entrada seja tokenizado corretamente e os tokens de sa√≠da sejam     decodificados com precis√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee6e2ef-1787-4559-b297-22dad5bd898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model, \n",
    "    tokenizer= tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c732544-0df0-491a-af71-5f224b262936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42371ec-7e6d-4d9f-a858-83005aa15cc2",
   "metadata": {},
   "source": [
    "## Par√¢metros para gera√ß√£o de texto\n",
    "\n",
    "Para personalizar o comportamento do nosso pipeline de gera√ß√£o de texto, podemos passar um <b>dicion√°rio de argumentos</b> para controlar v√°rios aspectos do processo de gera√ß√£o.\n",
    "\n",
    "`max_new_tokens` - Este par√¢metro especifica o n√∫mero m√°ximo de novos tokens a serem gerados em resposta ao prompt de entrada. Ele controla o comprimento do texto gerado.\n",
    "* Exemplo: Definir max_new_tokens como 500 significa que o modelo gerar√° at√© 500 tokens al√©m do prompt de entrada.\n",
    "\n",
    "`return_full_text` - Determina se deve retornar o texto completo, incluindo o prompt de entrada, ou apenas os tokens rec√©m-gerados.\n",
    "* Exemplo: Definir return_full_text como False significa que apenas os tokens rec√©m-gerados ser√£o retornados, excluindo o prompt de entrada original. Se definido como True, o texto retornado incluir√° o prompt de entrada e a continua√ß√£o gerada.\n",
    "\n",
    "`temperature` - Controla a aleatoriedade do processo de gera√ß√£o de texto. Valores mais baixos tornam a sa√≠da do modelo mais determin√≠stica e focada, enquanto valores mais altos aumentam a aleatoriedade e a criatividade.\n",
    "* Exemplo: Uma temperatura de 0,1 torna as previs√µes do modelo mais confi√°veis e menos variadas, levando a sa√≠das mais previs√≠veis. Uma temperatura mais alta resultaria em um texto mais diverso e variado.\n",
    "\n",
    "`do_sample` - Este par√¢metro habilita ou desabilita a amostragem (sampling) durante a gera√ß√£o de texto. Quando definido como True, o modelo faz a amostragem de tokens com base em suas probabilidades, adicionando um elemento de aleatoriedade √† sa√≠da. Quando definido como False, o modelo sempre escolhe o token de maior probabilidade (decodifica√ß√£o gananciosa / greedy decoding).\n",
    "* Exemplo: Definir do_sample como True permite uma gera√ß√£o de texto mais diversa e criativa. Se definido como False, a sa√≠da ser√° mais determin√≠stica, mas potencialmente menos interessante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7346066-1863-419b-9b13-88282b59abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.1, # 0.1 at√© 0.9\n",
    "    \"do_sample\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cfbe90-4a67-462b-bf25-eaebd9164098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e4f9491-215b-499c-b448-7a56e14e4207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Explique o que √© computa√ß√£o qu√¢ntica\"\n",
    "prompt = \"Quanto √© 7 x 6 - 42?\"\n",
    "\n",
    "output = pipe(prompt, **generation_args);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b9c175-0c43-4b9f-ab4c-52a554af7c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'generated_text': '\\n\\nOp√ß√µes de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\\n\\n\\n### Answer\\n\\nPara resolver a express√£o 7 x 6 - 42, precisamos seguir a ordem das opera√ß√µes, que √© frequentemente lembrada pelo acr√¥nimo PEMDAS (Par√™nteses, Expoentes, Multiplica√ß√£o e Divis√£o, e Adi√ß√£o e Subtra√ß√£o). Como n√£o h√° par√™nteses ou expoentes nesta express√£o, come√ßamos com a multiplica√ß√£o e depois a subtra√ß√£o.\\n\\nPasso 1: Realizar a multiplica√ß√£o\\n7 x 6 = 42\\n\\nPasso 2: Subtrair o resultado da multiplica√ß√£o de 42\\n42 - 42 = 0\\n\\nPortanto, a resposta para a express√£o 7 x 6 - 42 √© 0.'}],\n",
       " list)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f67178f-481d-430f-9c4c-bd8c81013738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Op√ß√µes de resposta: (A) 0 (B) 1 (C) 2 (D) 4 (E) 6\n",
      "\n",
      "\n",
      "### Answer\n",
      "\n",
      "Para resolver a express√£o 7 x 6 - 42, precisamos seguir a ordem das opera√ß√µes, que √© frequentemente lembrada pelo acr√¥nimo PEMDAS (Par√™nteses, Expoentes, Multiplica√ß√£o e Divis√£o, e Adi√ß√£o e Subtra√ß√£o). Como n√£o h√° par√™nteses ou expoentes nesta express√£o, come√ßamos com a multiplica√ß√£o e depois a subtra√ß√£o.\n",
      "\n",
      "Passo 1: Realizar a multiplica√ß√£o\n",
      "7 x 6 = 42\n",
      "\n",
      "Passo 2: Subtrair o resultado da multiplica√ß√£o de 42\n",
      "42 - 42 = 0\n",
      "\n",
      "Portanto, a resposta para a express√£o 7 x 6 - 42 √© 0. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb99a78-b00f-43da-bf59-dacb484498a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "### Answer:O Brasil √© o pa√≠s mais populoso da Am√©rica do Sul e o quarto maior do mundo, com uma popula√ß√£o estimada em mais de 210 milh√µes de pessoas. √â conhecido por sua rica biodiversidade, com uma grande variedade de paisagens, desde as florestas tropicais da Amaz√¥nia at√© as plan√≠cies do Pantanal e as montanhas da Serra do Espinha√ßo.\n",
      "\n",
      "A economia do Brasil √© diversificada, com setores importantes incluindo agricultura, minera√ß√£o, manufatura e servi√ßos. O pa√≠s √© um dos maiores produtores de a√ß√∫car, soja, caf√© e algod√£o, e tamb√©m √© um importante exportador de min√©rios como ferro, cobre e ouro.\n",
      "\n",
      "A cultura brasileira √© vibrante e influenciada por uma mistura de tradi√ß√µes ind√≠genas, africanas e europeias. A m√∫sica, como samba, bossa nova e pagode, √© mundialmente famosa, e o pa√≠s tamb√©m √© conhecido por seus desfiles de carnaval, que s√£o celebrados em todo o pa√≠s.\n",
      "\n",
      "A pol√≠tica brasileira √© caracterizada por um sistema de governo presidencialista, com o presidente como chefe de estado e governo. O pa√≠s tem um hist√≥rico complexo, com per√≠odos de prosperidade e instabilidade, mas recentemente tem feito progressos significativos em √°reas como educa√ß√£o, sa√∫de e desenvolvimento econ√¥mico.\n",
      "\n",
      "O Brasil tamb√©m √© conhecido por suas belezas naturais, incluindo a Praia do Forte, a Floresta Amaz√¥nica e a Serra do Capara√≥. O pa√≠s tem uma rica hist√≥ria e uma cultura diversificada, tornando-o um destino popular para turistas de todo o mundo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Me fale um pouco do Brasil\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "\n",
    "print(f\"{output[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be028c8-d55b-413c-a4b1-42a26906af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os tokens de paradas\n",
    "# obs: para cada modeo √© aconselh√°vel ver a documenta√ß√£o do modelo e sobre o tokens de parada ( end token )\n",
    "# Para este modelo vamos utilizar este tokens de parada\n",
    "\n",
    "\n",
    "# prompt = \"Explique o que √© computa√ß√£o qu√¢ntica\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "Voc√™ √© um assistente de ajuda.<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(prompt) # formata o prompt para o formato adequado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc85d50-8fb1-43c4-a6d4-c2443d53f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Voc√™ √© um assistente de ajuda.<|end|>\n",
      "<|user|>\n",
      "\"Me fale um pouco do Brasil\"<|end|>\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f77c5c-60fe-419e-a51f-10ecbb798ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nVoc√™ √© um assistente de ajuda.<|end|>\\n<|user|>\\n\"Me fale um pouco do Brasil\"<|end|>\\n<|assistant|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "899440d7-e982-4284-a419-96cf13a6aff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O Brasil √© o maior pa√≠s da Am√©rica do Sul e o quarto maior do mundo em extens√£o territorial, com uma √°rea de aproximadamente 8.515.767 quil√¥metros quadrados. Sua capital √© Bras√≠lia, inaugurada em 1960, e sua maior cidade √© S√£o Paulo, conhecida por sua rica hist√≥ria, cultura e economia din√¢mica. O pa√≠s √© conhecido por sua diversidade cultural, com mais de 200 grupos ind√≠genas e uma popula√ß√£o de mais de 210 milh√µes de pessoas. O Brasil tem uma economia vibrante e √© um dos maiores produtores de a√ß√∫car, caf√©, soja e petr√≥leo do mundo. Al√©m disso, o pa√≠s √© famoso por seus festivais, como o Carnaval, que √© celebrado em todo o pa√≠s e √© considerado um dos maiores eventos culturais do mundo.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab85c65-4757-4f33-821c-bc45e5c4c69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "665c9733-41bd-430a-acfd-798a2264ecc5",
   "metadata": {},
   "source": [
    "## Explorando a engenharia de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4cab414-8a99-42db-a06f-face29ed7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Voc√™ √© um assistente virtual prestativo. Responda as perguntas em portugu√™s.<|end|>\n",
      "<|user|>\n",
      "\"Descreva quais s√£o as principais bigtechs do mercado global nos dias tuais.\"<|end|>\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Descreva quais s√£o as principais bigtechs do mercado global nos dias tuais.\"\n",
    "sys_prompt = \"Voc√™ √© um assistente virtual prestativo. Responda as perguntas em portugu√™s.\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da4ea6a7-53a2-4bb4-9226-ad65f4862b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As principais bigtechs do mercado global s√£o empresas que operam em setores como tecnologia da informa√ß√£o, internet e intelig√™ncia artificial, e que t√™m uma influ√™ncia significativa em diversos aspectos da economia e da sociedade. Algumas das maiores e mais influentes bigtechs incluem:\n",
      "\n",
      "\n",
      "1. **Google**: Fundada em 1998, a Google √© uma empresa multinacional que desenvolve e vende produtos e servi√ßos relacionados √† internet, incluindo navegadores de internet, sistemas operacionais, software de produtividade, e-mail, servi√ßos de busca, entre outros.\n",
      "\n",
      "\n",
      "2. **Facebook (Meta Platforms)**: Fundada em 2004, o Facebook √© uma plataforma de rede social que tamb√©m oferece servi√ßos como Instagram, WhatsApp e Oculus VR. A Meta Platforms, que √© o nome atual da empresa ap√≥s a mudan√ßa de nome em 2021, est√° focada na realidade virtual e na tecnologia Meta.\n",
      "\n",
      "\n",
      "3. **Amazon**: Fundada em 1994, a Amazon √© uma empresa de com√©rcio eletr√¥nico que oferece uma ampla gama de produtos e servi√ßos, incluindo varejo, servi√ßos de streaming, servi√ßos de computa√ß√£o em nuvem, ecologia, entre outros.\n",
      "\n",
      "\n",
      "4. **Apple**: Fundada em 1976, a Apple √© uma empresa que desenvolve e vende produtos eletr√¥nicos, incluindo smartphones, tablets, computadores pessoais, e servi√ßos de software e hardware.\n",
      "\n",
      "\n",
      "5. **Microsoft**: Fundada em 1975, a Microsoft √© uma empresa que desenvolve e vende software, servi√ßos de computa√ß√£o em nuvem, jogos eletr√¥nicos, e hardware.\n",
      "\n",
      "\n",
      "Essas empresas s√£o consideradas bigtechs devido √† sua grande escala, poder de mercado, impacto econ√¥mico e influ√™ncia social. Elas t√™m sido alvo de escrut√≠nio regulat√≥rio e de discuss√µes sobre seu poder e responsabilidade no mercado global.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ddbd4f9-8b03-4295-911d-9dcb71b9f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A primeira lei da f√≠sica, tamb√©m conhecida como a lei da conserva√ß√£o da energia, foi formulada por Sir Isaac Newton. A lei afirma que a energia n√£o pode ser criada nem destru√≠da, apenas transformada de uma forma para outra. A express√£o matem√°tica da primeira lei pode ser escrita como:\n",
      "\n",
      "\n",
      "F = ma\n",
      "\n",
      "\n",
      "Onde F √© a for√ßa aplicada a um objeto, m √© a massa do objeto e a √© a acelera√ß√£o resultante. Essa equa√ß√£o √© fundamental para o campo da f√≠sica e tem implica√ß√µes em v√°rias √°reas, como mec√¢nica cl√°ssica, din√¢mica de fluidos e at√© mesmo na f√≠sica qu√¢ntica.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"quem formulou a primeira lei da f√≠sica?\"\n",
    "\n",
    "sys_prompt = \"Voc√™ √© um programador experiente. Retorne o c√≥digo requisitado e forne√ßa explica√ß√µes.\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95fefc-86eb-4b7b-9532-d387a5e1aa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2f1890-5210-4204-8ec0-2e887e4ad053",
   "metadata": {},
   "source": [
    "# Formato de Mensagens (LLMs em Chat)\n",
    "\n",
    "**Ideia central:** Em chats com LLMs, a conversa √© uma **lista de mensagens**.  \n",
    "Cada mensagem tem:\n",
    "- **`role`**: quem est√° falando (`system`, `user`, `assistant`)\n",
    "- **`content`**: o texto da mensagem\n",
    "\n",
    "## Por que usar esse formato?\n",
    "- O modelo entende **contexto de ida e volta** (v√°rios turnos).\n",
    "- Facilita controlar o comportamento (ex.: instru√ß√µes no `system`).\n",
    "\n",
    "## Estrutura do prompt (lista `msg`)\n",
    "Voc√™ envia algo como uma lista de dicion√°rios com `role` e `content`.  \n",
    "O modelo responde levando em conta **toda a conversa**, com foco na **√∫ltima** mensagem.\n",
    "\n",
    "### Exemplo m√≠nimo\n",
    "```json\n",
    "[\n",
    "  { \"role\": \"system\", \"content\": \"Voc√™ √© um assistente √∫til e direto.\" },\n",
    "  { \"role\": \"user\",   \"content\": \"Explique o que √© uma mensagem.\" }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d57b9f63-1618-4cc2-9dee-56898b2371d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Um prompt √© uma instru√ß√£o ou solicita√ß√£o que voc√™ d√° a um modelo de IA, como um chatbot ou uma intelig√™ncia artificial, para que ele possa entender e responder adequadamente. √â como uma dica ou um comando que ajuda o modelo a gerar uma resposta relevante.\n",
      "\n",
      "\n",
      "Por outro lado, um template √© um modelo pr√©-definido que cont√©m uma estrutura ou formato espec√≠fico para uma resposta. Ele pode incluir espa√ßos reservados ou marcadores que s√£o substitu√≠dos por informa√ß√µes espec√≠ficas quando o modelo de IA processa o prompt. Templates s√£o √∫teis para garantir que as respostas sejam consistentes e organizadas de acordo com um padr√£o espec√≠fico.\n",
      "\n",
      "\n",
      "Em resumo, um prompt √© uma instru√ß√£o para o modelo de IA, enquanto um template √© um modelo de resposta que ajuda a organizar e padronizar as respostas.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explique de uma forma simples o conceito de prompt e tamplet\"\n",
    "\n",
    "msg = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente virtual prestativo com conhecimentos e praticas de IA. Responda as perguntas em portugu√™s.\" },\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "\n",
    "output = pipe(msg, **generation_args)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a0ab-f2df-45ab-956b-4c485771aee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b31662-2ab0-4346-b1a3-7592d57d0b0c",
   "metadata": {},
   "source": [
    "### Otimizando com quantiza√ß√£o\n",
    "\n",
    "üß† Otimizando com Quantiza√ß√£o\n",
    "\n",
    "Modelos grandes podem ser dif√≠ceis de executar em m√°quinas com poucos recursos.Para contornar isso, usamos quantiza√ß√£o, uma t√©cnica que reduz o uso de mem√≥ria e acelera os c√°lculos ao representar pesos com menor precis√£o (peso com int8 ou 4 bits).\n",
    "\n",
    "Com o BitsAndBytesConfig da biblioteca transformers, √© poss√≠vel carregar modelos massivos de forma eficiente, sem grandes perdas de desempenho.\n",
    "\n",
    "‚öôÔ∏è Par√¢metros principais\n",
    "\n",
    "<b>load_in_4bit</b> ‚Üí Ativa a quantiza√ß√£o em 4 bits.\n",
    "Impacto: menor uso de mem√≥ria e infer√™ncia mais r√°pida.\n",
    "\n",
    "<b>bnb_4bit_quant_type</b> ‚Üí Define o tipo de quantiza√ß√£o; \"nf4\" (NormalFloat4) mant√©m o desempenho com menor precis√£o.\n",
    "Impacto: equil√≠brio entre tamanho e performance.\n",
    "\n",
    "<b>bnb_4bit_use_double_quant</b> ‚Üí Habilita a quantiza√ß√£o dupla.\n",
    "Impacto: reduz erros e melhora a estabilidade.\n",
    "\n",
    "<b>bnb_4bit_compute_dtype</b> ‚Üí Define o tipo de dado usado nos c√°lculos, como torch.bfloat16.\n",
    "Impacto: c√°lculos eficientes com m√≠nima perda de precis√£o.\n",
    "\n",
    "üöÄ <b><u>Resultado<u></b>\n",
    "\n",
    "Essas configura√ß√µes permitem rodar modelos grandes mesmo em hardwares limitados, tornando o processo mais leve e r√°pido, sem comprometer muito a qualidade do modelo.\n",
    "\n",
    "<b>Observa√ß√£o:</b> Se o modelo der erro de acesso (<u>Cannot access gated repo for url...</u>), use o seguinte comando no prompt:\n",
    "   - huggingface-cli login\n",
    "   - Cole seu token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58ecdd41-636d-4ed5-bcea-bdacda816d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de9accd8-7c98-40c9-b455-b29302773784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a05ab64fa24e00a06edde773347f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                 model_id, \n",
    "                                 quantization_config = quantization_config,\n",
    "                                 token = token # envia o token para o huggingface dar licensa para baixar o modelo em quest√£o  \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7843b2ee-b21c-4e2e-b230-4481d6023a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalmente, ao preparar dados para o LLM, usa-se tokenizer.apply_chat_template, que adiciona tokens EOS (fim de sequ√™ncia) ap√≥s cada resposta.\n",
    "\n",
    "prompt = \"Quem foi Dom Pedro II?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc385d31-e62a-4270-9ef0-1f946ad297c4",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black; width: 100%;\">\n",
    "\n",
    "#### <p>A fun√ß√£o  <u>tokenizer.apply_chat_template()</u>  da Hugging Face aplica automaticamente o formato correto de chat para o modelo escolhido, evitando erros manuais. O par√¢metro <u>return_tensors=\"pt\"</u> indica que os dados devem ser retornados como tensores PyTorch.</p>\n",
    "\n",
    "#### <u><b>As demais linhas de c√≥digo:</br></u> \n",
    "\n",
    "#### tokenizam as mensagens de entrada, movem os tensores para o dispositivo correto (GPU), geram novos tokens com base nos inputs fornecidos,</br> decodificam os tokens gerados de volta em texto leg√≠vel e finalmente retornam o texto gerado.</br>\n",
    "\n",
    "<hr style=\"border: 2px solid black; width: 100%;\">\n",
    "\n",
    "### O fluxo geral do c√≥digo √©:\n",
    "* `model_inputs = encodeds.to(device)` -  Move os tensores codificados para o dispositivo especificado (CPU ou GPU) para serem processados pelo modelo.\n",
    "\n",
    "* `encodeds` - Os tensores gerados na linha anterior.\n",
    "`to(device)` -  Move os tensores para o dispositivo especificado (device), que pode ser uma CPU ou GPU.\n",
    "\n",
    "* `generated_ids = model.generate...` -> Gera uma sequ√™ncia de tokens a partir dos model_inputs.\n",
    "     * model.generate: Fun√ß√£o do modelo que gera texto baseado nos inputs fornecidos.\n",
    "     * model_inputs: Os inputs processados, prontos para serem usados pelo modelo.\n",
    "     * max_new_tokens=1000: Limita a gera√ß√£o a no m√°ximo 1000 novos tokens.\n",
    "     * do_sample=True: Habilita amostragem aleat√≥ria durante a gera√ß√£o, o que pode resultar em sa√≠das mais variadas.\n",
    "     * pad_token_id=tokenizer.eos_token_id: Define o token de padding para ser o token de fim de sequ√™ncia, garantindo que a gera√ß√£o seja corretamente terminada.\n",
    "</br>\n",
    "\n",
    "* `decoded = tokenizer.batch_decode(generated_ids)` - decodifica os IDs gerados de volta para texto leg√≠vel.\n",
    " * `tokenizer.batch_decode` - fun√ß√£o que converte uma lista de IDs de tokens de volta para texto.\n",
    " * `generated_ids` - os IDs dos tokens gerados na etapa anterior.\n",
    "\n",
    "\n",
    "`res = decoded[0]` - extrai o primeiro item da lista de textos decodificados.\n",
    "decoded[0]: Pega o primeiro texto da lista decoded, que corresponde √† gera√ß√£o de texto para o primeiro (e possivelmente √∫nico) input fornecido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6cc9e93-c70b-4d0d-a524-bba86300efa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nQuem foi Dom Pedro II?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDom Pedro II foi o Imperador do Brasil entre 1831 e 1889. Ele nasceu em 2 de dezembro de 1825, em S√£o Crist√≥v√£o, na ent√£o Capitania de S√£o Crist√≥v√£o, no Brasil, e morreu em 5 de janeiro de 1891, em Porto Alegre, Rio Grande do Sul, Brasil.\\n\\nPedro II foi o segundo Imperador do Brasil, ap√≥s a abdica√ß√£o de seu pai, Dom Pedro I, que havia declarado a independ√™ncia do Brasil em 1822. Dom Pedro II ascendeu ao trono com apenas seis anos de idade, ap√≥s a morte de seu pai, e foi reinando por mais de 58 anos, at√© a Proclama√ß√£o da Rep√∫blica Brasileira em 1889.\\n\\nDurante seu reinado, Dom Pedro II implementou v√°rias reformas pol√≠ticas, econ√¥micas e sociais no Brasil, incluindo a aboli√ß√£o da escravatura em 1888. Ele tamb√©m promoveu o desenvolvimento da educa√ß√£o, da ci√™ncia e da arte no pa√≠s.\\n\\nDom Pedro II era conhecido por sua intelig√™ncia, sua cultura e sua habilidade em governar o pa√≠s de forma eficaz. Ele foi apelidado de \"O Pac√≠fico\" por sua habilidade em manter a paz no Brasil e em resolver conflitos diplom√°ticos.\\n\\nAinda hoje, Dom Pedro II √© considerado um dos imperadores mais importantes da hist√≥ria do Brasil, e sua figura √© lembrada com respeito e admira√ß√£o por muitos brasileiros.<|eot_id|>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encondeds = tokenizer.apply_chat_template(messages, return_tensors = \"pt\")\n",
    "model_input = encondeds.to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "                      model_input, \n",
    "                      max_new_tokens = 500,\n",
    "                      do_sample = True, \n",
    "                      pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "res = decoded[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ccfa0d2-0a5b-4930-bd66-c9bb50e3878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Quem foi Dom Pedro II?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Dom Pedro II foi o Imperador do Brasil entre 1831 e 1889. Ele nasceu em 2 de dezembro de 1825, em S√£o Crist√≥v√£o, na ent√£o Capitania de S√£o Crist√≥v√£o, no Brasil, e morreu em 5 de janeiro de 1891, em Porto Alegre, Rio Grande do Sul, Brasil.\n",
      "\n",
      "Pedro II foi o segundo Imperador do Brasil, ap√≥s a abdica√ß√£o de seu pai, Dom Pedro I, que havia declarado a independ√™ncia do Brasil em 1822. Dom Pedro II ascendeu ao trono com apenas seis anos de idade, ap√≥s a morte de seu pai, e foi reinando por mais de 58 anos, at√© a Proclama√ß√£o da Rep√∫blica Brasileira em 1889.\n",
      "\n",
      "Durante seu reinado, Dom Pedro II implementou v√°rias reformas pol√≠ticas, econ√¥micas e sociais no Brasil, incluindo a aboli√ß√£o da escravatura em 1888. Ele tamb√©m promoveu o desenvolvimento da educa√ß√£o, da ci√™ncia e da arte no pa√≠s.\n",
      "\n",
      "Dom Pedro II era conhecido por sua intelig√™ncia, sua cultura e sua habilidade em governar o pa√≠s de forma eficaz. Ele foi apelidado de \"O Pac√≠fico\" por sua habilidade em manter a paz no Brasil e em resolver conflitos diplom√°ticos.\n",
      "\n",
      "Ainda hoje, Dom Pedro II √© considerado um dos imperadores mais importantes da hist√≥ria do Brasil, e sua figura √© lembrada com respeito e admira√ß√£o por muitos brasileiros.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6403faeb-260f-447e-9c6d-44a01cf491d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be185db5-c6a6-443c-91ec-40f7b10bea26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
