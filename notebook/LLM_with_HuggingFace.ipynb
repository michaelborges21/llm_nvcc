{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3267b9ba-6e7c-406e-b9ac-7975b776d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -q transformers==4.48.2 einops accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9de0d-9463-4f98-9935-5c7c3205796f",
   "metadata": {},
   "source": [
    "## Importando as seguintes bibliotecas\n",
    "* <b>AutoModelForCausalLM:</b> Utilizado para carregar um modelo pr√©-treinado para gerar texto (escrever hist√≥rias, responder perguntas, completar c√≥digo, etc.) e n√£o quer se preocupar em encontrar a classe espec√≠fica para aquele modelo.\n",
    "* <b>AutoTokenizer:</b> Uma classe que fornece um tokenizador que corresponde ao modelo. O tokenizador √© respons√°vel por converter texto em tokens (num√©ricos) que o modelo pode entender.\n",
    "* <b>pipeline:</b> fornece uma interface simples e unificada para v√°rias tarefas de PLN, facilitando a execu√ß√£o de tarefas como gera√ß√£o de texto, classifica√ß√£o e tradu√ß√£o.\n",
    "* <b>BitsAndBytesConfig:</b> Uma classe para configura√ß√£o de quantiza√ß√£o e outras otimiza√ß√µes de baixo n√≠vel para melhorar a efici√™ncia computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4b5163-ccad-4ab3-9d51-f5a3ba8be52e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import getpass\n",
    "\n",
    "sys.path.append(os.path.abspath('../src')) # para a achar os modulos \n",
    "from keys import tokens\n",
    "from hw import processador\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36150ca-1c80-4552-900f-889015e130ee",
   "metadata": {},
   "source": [
    "## Leitura da chave do Hugging Face e verificando se o cuda est√° ativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b331ef-0aae-4d40-bc69-4b5e33fae3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome: NVIDIA GeForce RTX 3060\n",
      "Total de mem√≥ria: 11.62 GB\n",
      "Multiprocessadores: 28\n",
      "Capacidade de computa√ß√£o: 8.6\n",
      "\n",
      "Trabalhando com: cuda\n",
      "\n",
      ">>>TOKEN ACESSADO COM SUCESSO<<\n"
     ]
    }
   ],
   "source": [
    "device = processador()\n",
    "T = tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061398a-b37f-4341-ba5a-7be89840eec3",
   "metadata": {},
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0b1785-13bb-4b94-ab00-44d354f203bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d78801fd410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee433d-f155-4ef0-9822-c9dbf6e70a3c",
   "metadata": {},
   "source": [
    "## Carregando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e812671-fe0f-4b89-8cb9-b23e0a2c0027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c0e67e-1693-4e0a-b7fe-ee89a004d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801888c851c7415fa2c171bf61c1b72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740e2f0531554006ae3fa92f5cef4ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1224c7dbfe44424a88eeff624962c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2c253bb8424eac93fc51845640fba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66dc19e5d8848f69973eabd87871252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc428cdc237482c92997943eaf2021b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_model,            \n",
    "    device_map = device,        # Para acelerar o processo, o modelo usar√° a placa de v√≠deo (GPU) que o coputador oferece ou mesmo CPU.\n",
    "    torch_dtype = \"auto\",       # Define o tipo de dado ideal para o modelo rodar mais r√°pido e usar menos mem√≥ria.\n",
    "    trust_remote_code = True,   # Permite usar c√≥digo personalizado do Hugging Face, necess√°rio para modelos espec√≠ficos.\n",
    "    attn_implementation=\"eager\" # Escolhe um m√©todo de aten√ß√£o (\"eager\") para melhorar o desempenho de alguns modelos.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2904e-f4ed-430b-8bfd-06e003627de3",
   "metadata": {},
   "source": [
    "##  Criando o Tokinizador \n",
    "\n",
    "<p>O tokenizador traduz o texto em n√∫meros (tokens) para que o modelo de IA consiga process√°-lo.</br> Em seguida, ele converte os n√∫meros gerados pelo modelo de volta em texto que n√≥s podemos entender.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d84605-f8eb-4b75-89ea-c839f8a6e74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103568d9fc174ca29a08993d6266fec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b52bf20c4af4ad7a8c45aa583ddb85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e045f71bd61847e0aa7cd476d0fbfeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c364cea5244e45239876335d3cbb3f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b415072b7cc4e318f60da1026a06fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(id_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b527905-d895-43e5-8201-912a65ac4ac8",
   "metadata": {},
   "source": [
    "## Cria√ß√£o do Pipeline\n",
    "\n",
    "Par√¢metros:\n",
    "\n",
    "* \"text-generation\": especifica a tarefa que o pipeline est√° configurado para executar. Neste caso, estamos configurando um pipeline para gera√ß√£o de texto. O pipeline usar√° o modelo para gerar texto com base em um     prompt fornecido.\n",
    "* model=model: especifica o modelo pr√©-treinado que o pipeline usar√°. Aqui, estamos passando o model que carregamos anteriormente. Este modelo √© respons√°vel por gerar texto com base nos tokens de entrada.\n",
    "* tokenizer=tokenizer: especifica o tokenizador que o pipeline usar√°. Passamos o tokenizer que carregamos anteriormente para garantir que o texto de entrada seja tokenizado corretamente e os tokens de sa√≠da sejam     decodificados com precis√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee6e2ef-1787-4559-b297-22dad5bd898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model, \n",
    "    tokenizer= tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c732544-0df0-491a-af71-5f224b262936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42371ec-7e6d-4d9f-a858-83005aa15cc2",
   "metadata": {},
   "source": [
    "## Par√¢metros para gera√ß√£o de texto\n",
    "\n",
    "Para personalizar o comportamento do nosso pipeline de gera√ß√£o de texto, podemos passar um <b>dicion√°rio de argumentos</b> para controlar v√°rios aspectos do processo de gera√ß√£o.\n",
    "\n",
    "`max_new_tokens` - Este par√¢metro especifica o n√∫mero m√°ximo de novos tokens a serem gerados em resposta ao prompt de entrada. Ele controla o comprimento do texto gerado.\n",
    "* Exemplo: Definir max_new_tokens como 500 significa que o modelo gerar√° at√© 500 tokens al√©m do prompt de entrada.\n",
    "\n",
    "`return_full_text` - Determina se deve retornar o texto completo, incluindo o prompt de entrada, ou apenas os tokens rec√©m-gerados.\n",
    "* Exemplo: Definir return_full_text como False significa que apenas os tokens rec√©m-gerados ser√£o retornados, excluindo o prompt de entrada original. Se definido como True, o texto retornado incluir√° o prompt de entrada e a continua√ß√£o gerada.\n",
    "\n",
    "`temperature` - Controla a aleatoriedade do processo de gera√ß√£o de texto. Valores mais baixos tornam a sa√≠da do modelo mais determin√≠stica e focada, enquanto valores mais altos aumentam a aleatoriedade e a criatividade.\n",
    "* Exemplo: Uma temperatura de 0,1 torna as previs√µes do modelo mais confi√°veis e menos variadas, levando a sa√≠das mais previs√≠veis. Uma temperatura mais alta resultaria em um texto mais diverso e variado.\n",
    "\n",
    "`do_sample` - Este par√¢metro habilita ou desabilita a amostragem (sampling) durante a gera√ß√£o de texto. Quando definido como True, o modelo faz a amostragem de tokens com base em suas probabilidades, adicionando um elemento de aleatoriedade √† sa√≠da. Quando definido como False, o modelo sempre escolhe o token de maior probabilidade (decodifica√ß√£o gananciosa / greedy decoding).\n",
    "* Exemplo: Definir do_sample como True permite uma gera√ß√£o de texto mais diversa e criativa. Se definido como False, a sa√≠da ser√° mais determin√≠stica, mas potencialmente menos interessante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7346066-1863-419b-9b13-88282b59abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.1, # 0.1 at√© 0.9\n",
    "    \"do_sample\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cfbe90-4a67-462b-bf25-eaebd9164098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4f9491-215b-499c-b448-7a56e14e4207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explique o que √© computa√ß√£o qu√¢ntica\"\n",
    "#prompt = \"Quanto √© 7 x 6 - 42?\"\n",
    "\n",
    "output = pipe(prompt, **generation_args);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1b9c175-0c43-4b9f-ab4c-52a554af7c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'generated_text': '\\n\\nA. 0\\nB. 1\\nC. 3\\nD. 6\\n\\n# Answer\\nA. 0\\n\\nExplica√ß√£o:\\nPara resolver a express√£o 7 x 6 - 42, primeiro realizamos a multiplica√ß√£o e depois a subtra√ß√£o:\\n\\n7 x 6 = 42\\n\\nAgora subtra√≠mos 42 de 42:\\n\\n42 - 42 = 0\\n\\nPortanto, a resposta correta √© A. 0.'}],\n",
       " list)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f67178f-481d-430f-9c4c-bd8c81013738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A. 0\n",
      "B. 1\n",
      "C. 3\n",
      "D. 6\n",
      "\n",
      "# Answer\n",
      "A. 0\n",
      "\n",
      "Explica√ß√£o:\n",
      "Para resolver a express√£o 7 x 6 - 42, primeiro realizamos a multiplica√ß√£o e depois a subtra√ß√£o:\n",
      "\n",
      "7 x 6 = 42\n",
      "\n",
      "Agora subtra√≠mos 42 de 42:\n",
      "\n",
      "42 - 42 = 0\n",
      "\n",
      "Portanto, a resposta correta √© A. 0. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb99a78-b00f-43da-bf59-dacb484498a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "### Answer:O Brasil √© o maior pa√≠s da Am√©rica do Sul e o quarto maior do mundo em termos de √°rea terrestre e popula√ß√£o. Localizado na costa oeste do continente sul-americano, o Brasil tem uma variedade de paisagens, desde as plan√≠cies costeiras at√© as montanhas da Cordilheira dos Andes, e desde as florestas tropicais da Amaz√¥nia at√© as savanas do Cerrado e as √°reas semi√°ridas do Pantanal.\n",
      "\n",
      "A capital do Brasil √© Bras√≠lia, que foi inaugurada em 1960 e projetada pelo arquiteto Oscar Niemeyer e o urbanista L√∫cio Costa. A cidade foi constru√≠da para ser a nova capital do pa√≠s, movendo-a de Rio de Janeiro para facilitar o desenvolvimento do interior do pa√≠s.\n",
      "\n",
      "O Brasil tem uma rica hist√≥ria, que inclui a coloniza√ß√£o pelos portugueses no s√©culo XVI, a independ√™ncia do Imp√©rio Portugu√™s em 1822 e a transi√ß√£o para uma rep√∫blica em 1889. O pa√≠s tem sido um importante ator na pol√≠tica internacional, especialmente durante a era do imperialismo e a Segunda Guerra Mundial.\n",
      "\n",
      "A economia do Brasil √© diversificada, com setores importantes incluindo agricultura, minera√ß√£o, manufatura, servi√ßos e turismo. O pa√≠s √© um dos maiores produtores de a√ß√∫car, caf√©, soja, milho, cana-de-a√ß√∫car e min√©rio de ferro. O Brasil tamb√©m √© conhecido por suas belezas naturais, que atraem milh√µes de turistas a cada ano.\n",
      "\n",
      "O Brasil tem uma rica cultura, que inclui m√∫sica, dan√ßa, arte e culin√°ria. O pa√≠s √© conhecido por seus festivais, como o Carnaval, que √© celebrado em v√°rias cidades brasileiras com desfiles, m√∫sica e dan√ßa. O Brasil tamb√©m √© famoso por seus jogos de futebol, que s√£o seguidos com entusiasmo por pessoas de todo o mundo.\n",
      "\n",
      "Em termos de demografia, o Brasil tem uma popula√ß√£o diversificada, com mais de 200 grupos √©tnicos diferentes. A l√≠ngua oficial √© o portugu√™s, mas o pa√≠s tamb√©m tem uma grande popula√ß√£o de falantes de espanhol, especialmente no sul do pa√≠s.\n",
      "\n",
      "Em resumo, o Brasil √© um pa√≠s com uma rica hist√≥ria, uma economia diversificada e uma cultura vibrante. √â conhecido por suas belezas naturais, seu carnaval e seu futebol, e continua a ser um importante ator na pol√≠tica internacional.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Me fale um pouco do Brasil\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "\n",
    "print(f\"{output[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be028c8-d55b-413c-a4b1-42a26906af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os tokens de paradas\n",
    "# obs: para cada modeo √© aconselh√°vel ver a documenta√ß√£o do modelo e sobre o tokens de parada ( end token )\n",
    "# Para este modelo vamos utilizar este tokens de parada\n",
    "\n",
    "\n",
    "# prompt = \"Explique o que √© computa√ß√£o qu√¢ntica\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "Voc√™ √© um assistente de ajuda.<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(prompt) # formata o prompt para o formato adequado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc85d50-8fb1-43c4-a6d4-c2443d53f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Voc√™ √© um assistente de ajuda.<|end|>\n",
      "<|user|>\n",
      "\"Me fale um pouco do Brasil\"<|end|>\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f77c5c-60fe-419e-a51f-10ecbb798ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nVoc√™ √© um assistente de ajuda.<|end|>\\n<|user|>\\n\"Me fale um pouco do Brasil\"<|end|>\\n<|assistant|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899440d7-e982-4284-a419-96cf13a6aff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O Brasil √© o maior pa√≠s da Am√©rica do Sul e o quarto maior do mundo em termos de √°rea territorial. Localizado na costa oeste do Oceano Atl√¢ntico, o Brasil possui uma vasta paisagem que vai desde as praias tropicais do Nordeste at√© as montanhas da regi√£o Sul. A capital, Bras√≠lia, foi inaugurada em 1960 e √© conhecida por sua arquitetura moderna e planejamento urbano.\n",
      "\n",
      "\n",
      "A cultura brasileira √© uma mistura vibrante de tradi√ß√µes ind√≠genas, influ√™ncias africanas e europeias, especialmente portuguesa. A m√∫sica brasileira, como o samba e o bossa nova, √© mundialmente famosa, assim como a dan√ßa e a literatura. O pa√≠s tamb√©m √© conhecido por seus festivais, como o Carnaval, que √© celebrado em v√°rias cidades ao redor do mundo.\n",
      "\n",
      "\n",
      "A economia do Brasil √© diversificada, com setores importantes incluindo agricultura, minera√ß√£o, ind√∫stria e servi√ßos. O pa√≠s √© um dos maiores produtores de a√ß√∫car, caf√©, soja e min√©rio de ferro. O Brasil tamb√©m √© um dos maiores exportadores de petr√≥leo do mundo.\n",
      "\n",
      "\n",
      "A vida no Brasil √© caracterizada por uma rica diversidade cultural e uma variedade de costumes e tradi√ß√µes. Apesar de enfrentar desafios econ√¥micos e sociais, o Brasil continua a ser um pa√≠s com uma forte identidade e uma rica heran√ßa cultural.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab85c65-4757-4f33-821c-bc45e5c4c69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "665c9733-41bd-430a-acfd-798a2264ecc5",
   "metadata": {},
   "source": [
    "## Explorando a engenharia de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4cab414-8a99-42db-a06f-face29ed7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Voc√™ √© um assistente virtual prestativo. Responda as perguntas em portugu√™s.<|end|>\n",
      "<|user|>\n",
      "\"Descreva quais s√£o as principais bigtechs do mercado global nos dias tuais.\"<|end|>\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Descreva quais s√£o as principais bigtechs do mercado global nos dias tuais.\"\n",
    "sys_prompt = \"Voc√™ √© um assistente virtual prestativo. Responda as perguntas em portugu√™s.\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da4ea6a7-53a2-4bb4-9226-ad65f4862b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As principais bigtechs do mercado global s√£o empresas que operam em setores como tecnologia da informa√ß√£o, internet, intelig√™ncia artificial e finan√ßas digitais. Atualmente, algumas das maiores e mais influentes dessas empresas incluem:\n",
      "\n",
      "\n",
      "1. **Google (Alphabet Inc.)**: Fundada em 2004, a Google √© l√≠der no mercado de busca e tamb√©m possui uma grande participa√ß√£o em servi√ßos de publicidade online, produtos de software e hardware, e plataformas de m√≠dia.\n",
      "\n",
      "\n",
      "2. **Amazon.com, Inc.**: Fundada em 1994, a Amazon √© conhecida por ser um gigante do e-commerce, mas tamb√©m possui uma forte presen√ßa em servi√ßos de streaming de v√≠deo, servi√ßos de computa√ß√£o em nuvem (AWS), e dispositivos eletr√¥nicos.\n",
      "\n",
      "\n",
      "3. **Microsoft Corporation**: Fundada em 1975, a Microsoft √© uma gigante do software, com produtos como o Windows, Office, e tamb√©m tem uma forte presen√ßa em hardware, cloud computing (Azure), e jogos (Xbox).\n",
      "\n",
      "\n",
      "4. **Apple Inc.**: Fundada em 1976, a Apple √© l√≠der no mercado de smartphones com o iPhone, mas tamb√©m possui uma forte presen√ßa em computa√ß√£o pessoal (Mac), tablets (iPad), e servi√ßos de streaming (Apple Music).\n",
      "\n",
      "\n",
      "5. **Facebook, Inc. (Meta Platforms, Inc.)**: Fundada em 2004 como Facebook, agora conhecida como Meta Platforms, a empresa √© l√≠der no social networking e tamb√©m est√° expandindo seus servi√ßos para incluir realidade virtual e outras tecnologias inovadoras.\n",
      "\n",
      "\n",
      "6. **Tencent Holdings Ltd.**: Fundada em 1998, a Tencent √© uma gigante da China, com presen√ßa em jogos online, m√≠dia social (WeChat), e finan√ßas digitais (Tencent Pay).\n",
      "\n",
      "\n",
      "7. **Alibaba Group**: Fundada em 1999, a Alibaba √© l√≠der no mercado de e-commerce na China e tamb√©m possui presen√ßa em finan√ßas digitais (Ant Financial), tecnologia de IA, e servi√ßos de streaming.\n",
      "\n",
      "\n",
      "8. **ByteDance**: Fundada em 2012, a ByteDance √© conhecida por criar o TikTok, mas tamb√©m possui outras empresas de m√≠dia social e tecnologia de IA.\n",
      "\n",
      "\n",
      "Essas empresas s√£o consideradas \"bigtechs\" devido √† sua grande escala, impacto econ√¥mico e influ√™ncia sobre a sociedade e a economia global.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ddbd4f9-8b03-4295-911d-9dcb71b9f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A primeira lei da f√≠sica, tamb√©m conhecida como a lei da conserva√ß√£o da energia, foi formulada por Sir Isaac Newton. A lei afirma que a energia n√£o pode ser criada nem destru√≠da, apenas transformada de uma forma para outra. Essa lei √© fundamental para o campo da f√≠sica e tem implica√ß√µes em v√°rias √°reas, incluindo mec√¢nica cl√°ssica, termodin√¢mica e at√© mesmo na f√≠sica qu√¢ntica.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"quem formulou a primeira lei da f√≠sica?\"\n",
    "\n",
    "sys_prompt = \"Voc√™ √© um programador experiente. Retorne o c√≥digo requisitado e forne√ßa explica√ß√µes.\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95fefc-86eb-4b7b-9532-d387a5e1aa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2f1890-5210-4204-8ec0-2e887e4ad053",
   "metadata": {},
   "source": [
    "# Formato de Mensagens (LLMs em Chat)\n",
    "\n",
    "**Ideia central:** Em chats com LLMs, a conversa √© uma **lista de mensagens**.  \n",
    "Cada mensagem tem:\n",
    "- **`role`**: quem est√° falando (`system`, `user`, `assistant`)\n",
    "- **`content`**: o texto da mensagem\n",
    "\n",
    "## Por que usar esse formato?\n",
    "- O modelo entende **contexto de ida e volta** (v√°rios turnos).\n",
    "- Facilita controlar o comportamento (ex.: instru√ß√µes no `system`).\n",
    "\n",
    "## Estrutura do prompt (lista `msg`)\n",
    "Voc√™ envia algo como uma lista de dicion√°rios com `role` e `content`.  \n",
    "O modelo responde levando em conta **toda a conversa**, com foco na **√∫ltima** mensagem.\n",
    "\n",
    "### Exemplo m√≠nimo\n",
    "```json\n",
    "[\n",
    "  { \"role\": \"system\", \"content\": \"Voc√™ √© um assistente √∫til e direto.\" },\n",
    "  { \"role\": \"user\",   \"content\": \"Explique o que √© uma mensagem.\" }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d57b9f63-1618-4cc2-9dee-56898b2371d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Um prompt √© uma instru√ß√£o ou solicita√ß√£o que voc√™ d√° a um modelo de IA, como um chatbot ou uma intelig√™ncia artificial, para que ele possa entender e responder adequadamente. √â como uma dica ou um ponto de partida para a IA come√ßar a processar a informa√ß√£o e gerar uma resposta.\n",
      "\n",
      "\n",
      "Por outro lado, um template √© um modelo pr√©-definido que ajuda a organizar e estruturar a resposta de uma IA. Ele cont√©m um formato espec√≠fico ou uma sequ√™ncia de passos que a IA deve seguir para gerar uma resposta adequada. Templates podem ser usados para garantir consist√™ncia e clareza nas respostas, especialmente quando se trata de informa√ß√µes repetitivas ou comuns.\n",
      "\n",
      "\n",
      "Em resumo, um prompt √© como uma dica para a IA, enquanto um template √© como um guia para a IA seguir ao gerar uma resposta.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explique de uma forma simples o conceito de prompt e tamplet\"\n",
    "\n",
    "msg = [\n",
    "    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente virtual prestativo com conhecimentos e praticas de IA. Responda as perguntas em portugu√™s.\" },\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "\n",
    "output = pipe(msg, **generation_args)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a0ab-f2df-45ab-956b-4c485771aee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b31662-2ab0-4346-b1a3-7592d57d0b0c",
   "metadata": {},
   "source": [
    "### Otimizando com quantiza√ß√£o\n",
    "\n",
    "üß† Otimizando com Quantiza√ß√£o\n",
    "\n",
    "Modelos grandes podem ser dif√≠ceis de executar em m√°quinas com poucos recursos.Para contornar isso, usamos quantiza√ß√£o, uma t√©cnica que reduz o uso de mem√≥ria e acelera os c√°lculos ao representar pesos com menor precis√£o (peso com int8 ou 4 bits).\n",
    "\n",
    "Com o BitsAndBytesConfig da biblioteca transformers, √© poss√≠vel carregar modelos massivos de forma eficiente, sem grandes perdas de desempenho.\n",
    "\n",
    "‚öôÔ∏è Par√¢metros principais\n",
    "\n",
    "<b>load_in_4bit</b> ‚Üí Ativa a quantiza√ß√£o em 4 bits.\n",
    "Impacto: menor uso de mem√≥ria e infer√™ncia mais r√°pida.\n",
    "\n",
    "<b>bnb_4bit_quant_type</b> ‚Üí Define o tipo de quantiza√ß√£o; \"nf4\" (NormalFloat4) mant√©m o desempenho com menor precis√£o.\n",
    "Impacto: equil√≠brio entre tamanho e performance.\n",
    "\n",
    "<b>bnb_4bit_use_double_quant</b> ‚Üí Habilita a quantiza√ß√£o dupla.\n",
    "Impacto: reduz erros e melhora a estabilidade.\n",
    "\n",
    "<b>bnb_4bit_compute_dtype</b> ‚Üí Define o tipo de dado usado nos c√°lculos, como torch.bfloat16.\n",
    "Impacto: c√°lculos eficientes com m√≠nima perda de precis√£o.\n",
    "\n",
    "üöÄ <b><u>Resultado<u></b>\n",
    "\n",
    "Essas configura√ß√µes permitem rodar modelos grandes mesmo em hardwares limitados, tornando o processo mais leve e r√°pido, sem comprometer muito a qualidade do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ecdd41-636d-4ed5-bcea-bdacda816d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9accd8-7c98-40c9-b455-b29302773784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0a950f0d9a4d82baaa080e6904ba42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69aecaf49c84e5a8a82cb65c558a184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8e31a122a54aeea95d6c0f971e9c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec150acbe2742e0a0312efaee1e3b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af7f860a8644743a55bf888eaaaabbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d140e430934841ee9f471a1c8ba9da8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a9b056b80c458bba5ee8e0c60f95a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57f25e526c44ddeab2574d0775dab86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d408ec75529d447e90ea548f7b91affd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(T)\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                 model_id, \n",
    "                                 quantization_config = quantization_config,\n",
    "                                 # token = T # envia o token para o huggingface dar licensa para baixar o modelo em quest√£o  \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8caf01-a133-481b-88e3-381c6a88cd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc385d31-e62a-4270-9ef0-1f946ad297c4",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid black; width: 100%;\">\n",
    "\n",
    "#### <p>A fun√ß√£o  <u>tokenizer.apply_chat_template()</u>  da Hugging Face aplica automaticamente o formato correto de chat para o modelo escolhido, evitando erros manuais. O par√¢metro <u>return_tensors=\"pt\"</u> indica que os dados devem ser retornados como tensores PyTorch.</p>\n",
    "\n",
    "#### <u><b>As demais linhas de c√≥digo:</br></u> \n",
    "\n",
    "#### tokenizam as mensagens de entrada, movem os tensores para o dispositivo correto (GPU), geram novos tokens com base nos inputs fornecidos,</br> decodificam os tokens gerados de volta em texto leg√≠vel e finalmente retornam o texto gerado.</br>\n",
    "\n",
    "<hr style=\"border: 2px solid black; width: 100%;\">\n",
    "\n",
    "### O fluxo geral do c√≥digo √©:\n",
    "* `model_inputs = encodeds.to(device)` -  Move os tensores codificados para o dispositivo especificado (CPU ou GPU) para serem processados pelo modelo.\n",
    "\n",
    "* `encodeds` - Os tensores gerados na linha anterior.\n",
    "`to(device)` -  Move os tensores para o dispositivo especificado (device), que pode ser uma CPU ou GPU.\n",
    "\n",
    "* `generated_ids = model.generate...` -> Gera uma sequ√™ncia de tokens a partir dos model_inputs.\n",
    "     * model.generate: Fun√ß√£o do modelo que gera texto baseado nos inputs fornecidos.\n",
    "     * model_inputs: Os inputs processados, prontos para serem usados pelo modelo.\n",
    "     * max_new_tokens=1000: Limita a gera√ß√£o a no m√°ximo 1000 novos tokens.\n",
    "     * do_sample=True: Habilita amostragem aleat√≥ria durante a gera√ß√£o, o que pode resultar em sa√≠das mais variadas.\n",
    "     * pad_token_id=tokenizer.eos_token_id: Define o token de padding para ser o token de fim de sequ√™ncia, garantindo que a gera√ß√£o seja corretamente terminada.\n",
    "</br>\n",
    "\n",
    "* `decoded = tokenizer.batch_decode(generated_ids)` - decodifica os IDs gerados de volta para texto leg√≠vel.\n",
    " * `tokenizer.batch_decode` - fun√ß√£o que converte uma lista de IDs de tokens de volta para texto.\n",
    " * `generated_ids` - os IDs dos tokens gerados na etapa anterior.\n",
    "\n",
    "\n",
    "`res = decoded[0]` - extrai o primeiro item da lista de textos decodificados.\n",
    "decoded[0]: Pega o primeiro texto da lista decoded, que corresponde √† gera√ß√£o de texto para o primeiro (e possivelmente √∫nico) input fornecido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7843b2ee-b21c-4e2e-b230-4481d6023a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalmente, ao preparar dados para o LLM, usa-se tokenizer.apply_chat_template, que adiciona tokens EOS (fim de sequ√™ncia) ap√≥s cada resposta.\n",
    "\n",
    "prompt = \"Quem foi Dom Pedro II?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6cc9e93-c70b-4d0d-a524-bba86300efa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nQuem foi Dom Pedro II?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDom Pedro II foi o Imperador do Brasil entre 1831 e 1889. Ele nasceu em 2 de dezembro de 1825, em S√£o Crist√≥v√£o, na ent√£o Capitania de S√£o Crist√≥v√£o, no Brasil, e morreu em 5 de janeiro de 1891, em Porto Alegre, Rio Grande do Sul, Brasil.\\n\\nPedro II foi o segundo Imperador do Brasil, ap√≥s a abdica√ß√£o de seu pai, Dom Pedro I, que havia declarado a independ√™ncia do Brasil em 1822. Dom Pedro II ascendeu ao trono com apenas seis anos de idade, ap√≥s a morte de seu pai, e foi reinando por mais de 58 anos, at√© a Proclama√ß√£o da Rep√∫blica Brasileira em 1889.\\n\\nDurante seu reinado, Dom Pedro II implementou v√°rias reformas pol√≠ticas, econ√¥micas e sociais no Brasil, incluindo a aboli√ß√£o da escravatura em 1888. Ele tamb√©m promoveu o desenvolvimento da educa√ß√£o, da ci√™ncia e da arte no pa√≠s.\\n\\nDom Pedro II era conhecido por sua intelig√™ncia, sua cultura e sua habilidade em governar o pa√≠s de forma eficaz. Ele foi apelidado de \"O Pac√≠fico\" por sua habilidade em manter a paz no Brasil e em resolver conflitos diplom√°ticos.\\n\\nAinda hoje, Dom Pedro II √© considerado um dos imperadores mais importantes da hist√≥ria do Brasil, e sua figura √© lembrada com respeito e admira√ß√£o por muitos brasileiros.<|eot_id|>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encondeds = tokenizer.apply_chat_template(messages, return_tensors = \"pt\")\n",
    "model_input = encondeds.to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "                      model_input, \n",
    "                      max_new_tokens = 500,\n",
    "                      do_sample = True, \n",
    "                      pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "res = decoded[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ccfa0d2-0a5b-4930-bd66-c9bb50e3878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Quem foi Dom Pedro II?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Dom Pedro II foi o Imperador do Brasil entre 1831 e 1889. Ele nasceu em 2 de dezembro de 1825, em S√£o Crist√≥v√£o, na ent√£o Capitania de S√£o Crist√≥v√£o, no Brasil, e morreu em 5 de janeiro de 1891, em Porto Alegre, Rio Grande do Sul, Brasil.\n",
      "\n",
      "Pedro II foi o segundo Imperador do Brasil, ap√≥s a abdica√ß√£o de seu pai, Dom Pedro I, que havia declarado a independ√™ncia do Brasil em 1822. Dom Pedro II ascendeu ao trono com apenas seis anos de idade, ap√≥s a morte de seu pai, e foi reinando por mais de 58 anos, at√© a Proclama√ß√£o da Rep√∫blica Brasileira em 1889.\n",
      "\n",
      "Durante seu reinado, Dom Pedro II implementou v√°rias reformas pol√≠ticas, econ√¥micas e sociais no Brasil, incluindo a aboli√ß√£o da escravatura em 1888. Ele tamb√©m promoveu o desenvolvimento da educa√ß√£o, da ci√™ncia e da arte no pa√≠s.\n",
      "\n",
      "Dom Pedro II era conhecido por sua intelig√™ncia, sua cultura e sua habilidade em governar o pa√≠s de forma eficaz. Ele foi apelidado de \"O Pac√≠fico\" por sua habilidade em manter a paz no Brasil e em resolver conflitos diplom√°ticos.\n",
      "\n",
      "Ainda hoje, Dom Pedro II √© considerado um dos imperadores mais importantes da hist√≥ria do Brasil, e sua figura √© lembrada com respeito e admira√ß√£o por muitos brasileiros.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6403faeb-260f-447e-9c6d-44a01cf491d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be185db5-c6a6-443c-91ec-40f7b10bea26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
